---
title: "Analyzing the Relationship Between MPG and Transmission"
author: "Matiki"
date: "August 29, 2018"
output: 
  html_document: 
    keep_md: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
# Load all necessary R packages into current session
library(ggplot2)
library(dplyr)
library(GGally)
library(gridExtra)
```

## Executive Summary
We are going to investigate MPG for various vehicles using the mtcars dataset. 
We would like to know how the variables affect MPG, and in particular, we want 
to investigate the relationship between MPG and transmission. We are tasked with
answering the following questions:

1) Is manual or automatic transmission better for MPG?
2) Can we quantify the MPG difference between automatic and manual transmission?

To answer these questions we will use both simple linear regression and also a 
multivariate linear regression to model the relationship. Since there are 
several variables we could include in many different linear models we could fit,
we will use a backwards elimination method to attempt to find the best fitting 
linear regression model for our given data.

## Exploratory analysis
We'll start by taking a look at the data and doing some exploratory analysis. 
We first will load the mtcars data and take a look at the first few rows. 
Then we'll peek at the structure and get a quick summary of the data. For more 
information about the data we can look at the help file.

```{r}
# Read data into R
data("mtcars")

# Take a look at the data
head(mtcars)
str(mtcars)
summary(mtcars)
#?mtcars
```

It seems **am** and **vs** are binary factor variables, and **cyl**, **gear**, & **carb** are multi-
level factor variables, since they can only take integer values they are not 
continuous. However for simplicity we will treat them as continuous variables 
in our model. We'll convert the desired variables into factor variables.

```{r}
# Convert to factor variables
mtcars$vs <- as.factor(mtcars$vs)
mtcars$am <- as.factor(mtcars$am)

# Check that the changers took hold
str(mtcars)
```

We'll explore the data a bit further to help inform our modeling process. First 
let's see how the variables in **mtcars** are correlated to each other.

```{r}
# Get correlation matrix of the continuous variables
mtcars[, c(1, 3:7)] %>% 
        ggcorr(label = T)
```

It seems many of the variables show strong correlation to the other variables, 
so it is likely not all of them will be used in the final model, otherwise we 
might have overfitting.

Let's check to see how the **mpg** data are distributed. We'll plot the density 
along with a red line for the mean and blue lines representing the 95% confidence
intervals.

```{r}
# Check to see if mpg is normally distributed
ggplot(mtcars, aes(mpg)) +
        geom_density() +
        xlim(0, 50) + 
        geom_vline(xintercept = mean(mtcars$mpg),
                   color = "red",
                   linetype = "dashed") +
        geom_vline(xintercept = mean(mtcars$mpg) - 2 * sd(mtcars$mpg),
                   color = "blue",
                   linetype = "dashed") +
        geom_vline(xintercept = mean(mtcars$mpg) + 2 * sd(mtcars$mpg),
                   color = "blue",
                   linetype = "dashed") 
```

Looks like the data may not be normally distributed. It appears slightly skewed
and it might be tail heavy. This could throw off our model a bit, but for now 
we'll continue.

## Inference & Hypothesis Testing

Now we'll plot **mpg** vs **am** to visualize how the two are related, and then
look the mean mpg for cars with manual and automatic transmissions.

```{r}
# Plot mpg vs am
ggplot(mtcars,
       aes(x = am,
           y = mpg)) +
        geom_point()

# Look at the mean mpg for cars with/without automatic 
mtcars %>% group_by(am) %>%
        summarize(mean = mean(mpg))
```

So it looks like there's a difference in average mpg for automatic and manual 
transmission cars. Cars with automatic transmission have an average mpg of 17.1
and those with manual transmission have an average mpg of 24.4

However the two groups do not appear to have the same variance, so let's keep 
this in mind while running a hypothesis test to see if this difference may be
statistically significant. We'll perform a two group T-test and we will choose 
our alpha to be 0.05.

```{r}
# Welch Two Sample T-test
t.test(mpg ~ am, data= mtcars, 
       var.equal = FALSE, paired=FALSE, conf.level = .95)
```

So it seems manual transmission is associated with an increase in mpg of 7.25, 
and this is significant with a p-value of 0.001374.

## Model Selection
We'll start by looking at the simple linear model regressing **mpg** on **am** alone

```{r}
# Preliminary model fit: mpg ~ am
fit1 <- lm(mpg ~ am, mtcars)

# Take a look at the model/coefficients
summary(fit1)$coef
```

This confirms our previous analysis, here the coefficient labeled 'intercept' 
represents the mean mpg for automatic transmission cars, and the coefficient 
'am1' represents the change in mean for cars with manual transmission. It appears
to be an increase of about 7.25, like we saw before. The p-value here is different
because by default, R's 'lm' function uses a test assuming equal variance in the
two groups.

Now let's look at the multivariate regression model using all the variables.
```{r}
# fit regression model with all regressors
fit_all <- lm(mpg ~ ., mtcars)
summary(fit_all)
```

The change in mpg no longer appears to be significant, but our model is likely 
not the best considering how much correlation we saw among the regressors earlier.
We will use a step-wise backwards elimination method to remove regressors one-by-one
until we have something that better models our data. We'll look at all the 
variable's coefficients and choose the one with the highest p-value to eliminate
from our next model. We will repeat this process until all our coefficients are 
significant.

```{r}
fit2 <- lm(mpg ~ . - cyl, mtcars)
summary(fit2)

fit3 <- lm(mpg ~ . - cyl - vs, mtcars)
summary(fit3)

fit4 <- lm(mpg ~ . - cyl - vs - carb, mtcars)
summary(fit4)

fit5 <- lm(mpg ~ . - cyl - vs - carb - gear, mtcars)
summary(fit5)

fit6 <- lm(mpg ~ . - cyl - vs - carb - gear - drat, mtcars)
summary(fit6)

fit7 <- lm(mpg ~ . - cyl - vs - carb - gear - drat - disp, mtcars)
summary(fit7)

fit8 <- lm(mpg ~ . - cyl - vs - carb - gear - drat - disp - hp, mtcars)
summary(fit8)
```

We can see that the backwards elmination method produces a model which regresses
**mpg** on **am**, **wt**, and **qsec**. But it seems that the R^2 value actually
decreased after in our final two models, so let's do some more analysis to make 
sure we have good model fit.

```{r}
fit_wtqsec <- lm(mpg ~ am + wt + qsec, mtcars)

fit_wtqsechp <- lm(mpg ~ am + wt + qsec + hp, mtcars)

fit_wtqsechpdisp <- lm(mpg ~ am + wt + qsec + hp + disp, mtcars)

anova(fit_wtqsec, fit_wtqsechp, fit_wtqsechpdisp)
```

This shows that there is not a statistically significant improvement in the model
if we include **hp** and **disp** as regressors. Therefore we will choose to 
include only **am**, **wt**, and **qsec** in our final model.

## Diagnostics
Now we will run some diagnostics to see how well our model fits the data. We'll
plot the residuals agains the fitted values, a Normal Q-Q plot, and a 
scale-location plot.

```{r}
fit_final <- lm(mpg ~ am + wt + qsec, mtcars)

# Residuals vs fitted values
g1 <- ggplot(fit_final,
       aes(x = .fitted, y = .resid)) +
        geom_point() +
        geom_smooth() + 
        geom_hline(yintercept = 0,
                   col = "red",
                   linetype = "dashed") +
        labs(title = "Residuals vs Fitted Values",
             x = "Fitted Values",
             y = "Residuals")

# Normal QQ plot
g2 <- ggplot(fit_final) +
        geom_qq(aes(sample = .stdresid)) +
        geom_abline(intercept = 0, slope = 1, 
                    linetype = "dashed", col = "red") +
        labs(title = "Normal Q-Q Plot",
             x = "Theoretical Quantiles",
             y = "Standardized Residuals") 

# Scale location plot
g3 <- ggplot(fit_final,
       aes(y = sqrt(abs(.stdresid)),
           x = .fitted)) + 
        geom_point() +
        geom_smooth() +
        labs(title = "Scale Location Plot",
             x = "Fitted Values",
             y = expression(sqrt("|Standardized residuals|")))

grid.arrange(g1, g2, g3, ncol = 3)
```

We don't notice any particular pattern among these plots that might indicate a 
linear regression model was a poor choice. We do notice that the data might not
be perfectly normally distributed, as we saw earlier in our exploratory analysis,
which might throw off our results. However we are still reasonably close so we 
can accept some error.

## Conclusion

```{r}
summary(fit_final)
confint(fit_final)
```

Based on our multivariable linear regression model, we expect on average, cars 
with manual transmission to get 2.94 mpg more than cars with automatic transmission, 
while holding other regressors fixed. Our estimate is statistically significant 
for alpha = 0.05, and has a p-value of 0.0467. 

We can construct a 95% confidence interval and see that we are 95% confident 
that our estimate of the increase in mpg in manual transmission cars lies 
between 0.0457 and 5.823.

Therefore we conclude that manual transmission is associated with better mpg than
automatic transmission.